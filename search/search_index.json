{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MODERATE","text":"<p>Welcome to the documentation of the MODERATE platform </p> <p>The MODERATE project is funded under the Horizon Europe program. Its main objective is to develop services and tools that focus on building-related data sources and datasets.</p> <p>MODERATE consists of three main components:</p> <ul> <li>A collection of well-curated public datasets related to the building sector</li> <li>A suite of web applications and CLI tools designed to address specific challenges in building data analysis (see Tools and Services for details)</li> <li>The MODERATE Platform at www.moderate.cloud: a central web application that serves as both an entry point to various tools and a catalog for uploading, browsing, and downloading datasets, along with other proof-of-concept features</li> </ul>"},{"location":"#what-can-moderate-do-for-you","title":"What can MODERATE do for you?","text":"<p>While MODERATE is still in its early stages as a research project, we're excited to share several valuable resources that might benefit your work:</p> <ul> <li>Explore our tools and services: While some are specialized for specific use cases, they represent cutting-edge solutions that could help address your current challenges</li> <li>Adapt our tools to your needs: Even if our tools don't perfectly match your use case, you can fork them and build upon our solid foundation</li> <li>Discover our Platform UI: We offer user-friendly tools for data exploration, dataset management, and synthetic data generation</li> <li>Access curated datasets: We maintain a small collection of high-quality datasets available through both GitHub and our Platform UI, ready for your research and innovation projects</li> </ul> <p>For more information</p> <p>For additional context about the MODERATE project, including team information and extended documentation, visit our main site.</p>"},{"location":"developer-documentation/","title":"Developer documentation","text":"<p>This section contains design decisions, guides, and tutorials intended for the internal MODERATE development team or other technically oriented developers interacting with the MODERATE platform at a low level.</p> <p>These decisions are kept alongside more publicly oriented documentation for traceability purposes and to centralize information. If you are an end user seeking to learn more about MODERATE's tools and services, this subsection is likely outside your scope. Additionally, some of these design decisions may evolve or change over time, and the pages within risk becoming outdated.</p>"},{"location":"developer-documentation/bento/","title":"Deploying ML Models","text":"<p>The MODERATE platform leverages Yatai to offer a scalable service for storing and exposing machine learning models created with BentoML. This allows developers to utilize the MODERATE platform as a model repository, as well as a service that can automatically deploy HTTP APIs for said models.</p> <p>This guide provides a concise example of how to upload and deploy a machine learning model on the platform from start to finish.</p> <p>About BentoML</p> <p>Please check the BentoML quickstart tutorial for an introduction to the main concepts behind BentoML.</p> <p>The first step is to clone the BentoML Git repository:</p> <pre><code>$ git clone --depth 1 --branch v1.0.18 git@github.com:bentoml/BentoML.git # (1)!\n</code></pre> <ol> <li>We clone a specific version for reproducibility. In any case, this guide should work for later versions.</li> </ol> <p>Create a virtualenv and install the Python package requirements of the quickstart example:</p> <pre><code>$ cd BentoML/examples/quickstart\n$ virtualenv --python python3 .venv\n$ source .venv/bin/activate\n$ pip install -r requirements.txt\n</code></pre> <p>Train the model and save it to the BentoML local model store:</p> <pre><code>$ python train.py\nModel saved: Model(tag=\"iris_clf:5ehbqtxctcr7gfrz\")\n</code></pre> <p>Ignore the virtualenv</p> <p>Make sure to add <code>.venv</code> to the <code>.bentoignore</code> to avoid packing the entire virtualenv into the Bento.</p> .bentoignore<pre><code>__pycache__/\n*.py[cod]\n*$py.class\n.ipynb_checkpoints\n.venv\n</code></pre> <p>The <code>build</code> command creates the Bento: a self-contained package that contains all assets that are necessary to deploy the model:</p> <pre><code>$ bentoml build\nBuilding BentoML service \"iris_classifier:mkac3gxctsbd4frz\" from build context \"[...]/BentoML/examples/quickstart\".\nPacking model \"iris_clf:5ehbqtxctcr7gfrz\"\nLocking PyPI package versions.\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2557\u2591\u2591\u2591\u2588\u2588\u2588\u2557\u2588\u2588\u2557\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2551\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557\u2591\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2557\u2591\u2591\u2588\u2588\u2554\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d\u2591\u2591\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2591\u2591\u2591\u2591\u2591\n\u2588\u2588\u2588\u2588\u2588\u2588\u2566\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2591\u255a\u2588\u2588\u2588\u2551\u2591\u2591\u2591\u2588\u2588\u2551\u2591\u2591\u2591\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2591\u255a\u2550\u255d\u2591\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u2591\u2591\u255a\u2550\u2550\u255d\u2591\u2591\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u255a\u2550\u2550\u2550\u2550\u255d\u2591\u255a\u2550\u255d\u2591\u2591\u2591\u2591\u2591\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nSuccessfully built Bento(tag=\"iris_classifier:mkac3gxctsbd4frz\").\n\nPossible next steps:\n\n * Containerize your Bento with `bentoml containerize`:\n    $ bentoml containerize iris_classifier:mkac3gxctsbd4frz\n\n * Push to BentoCloud with `bentoml push`:\n    $ bentoml push iris_classifier:mkac3gxctsbd4frz\n</code></pre> <p>The <code>list</code> command provides a list of the local Bentos:</p> <pre><code>$ bentoml list\n Tag                               Size       Creation Time        Path\n iris_classifier:mkac3gxctsbd4frz  25.34 KiB  2023-04-24 14:34:46  ~/bentoml/bentos/iris_classifier/mkac3gxctsbd4frz\n</code></pre> <p>After the Bento is added to the system, it can be deployed to MODERATE's Yatai instance. To do this, first, you will need to log in using the API token provided by the platform administrator:</p> <pre><code>$ bentoml yatai login --api-token &lt;token&gt; --endpoint https://yatai.moderate.cloud\nOverriding existing Yatai context config: default\nSuccessfully logged in as user \"User\" in organization \"default\".\n</code></pre> <p>Yatai serves as a storage platform for Bentos and Models, much like how an image registry functions for container images. To upload a Bento to the Yatai instance, simply use the <code>push</code> command to transfer it from the system:</p> <pre><code>$ bentoml push iris_classifier:mkac3gxctsbd4frz\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Successfully pushed model \"iris_clf:5ehbqtxctcr7gfrz\"                                                                                                                                                           \u2502\n\u2502 Successfully pushed bento \"iris_classifier:mkac3gxctsbd4frz\"                                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nPushing Bento \"iris_classifier:mkac3gxctsbd4frz\" \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 7.2/7.2 kB \u2022 ? \u2022 0:00:00\n     Uploading model \"iris_clf:5ehbqtxctcr7gfrz\" \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.0% \u2022 2.1/2.1 kB \u2022 ? \u2022 0:00:00\n</code></pre> <p>Once you have successfully uploaded a Bento to Yatai, it will be displayed in the Bentos section of the dashboard:</p> <p></p> <p>Number of replicas</p> <p>In the following example both <code>min_replicas</code> and <code>max_replicas</code> are set to <code>1</code> for development purposes. In a production environment it would be better for <code>max_replicas</code> to be <code>2</code> or <code>3</code> to provide some availability guarantees.</p> <p>Resource requests</p> <p>Please reduce the resource requests of the Service and Runner replicas if possible. These values can be defined in <code>targets.*.config.resources.requests</code> and <code>targets.*.config.runners.*.resources.requests</code> respectively.</p> <p>Endpoint public access</p> <p>Note that if you activate Endpoint public access in the Deployment form, the HTTP API will be exposed to the Internet without authentication. This is the equivalent of setting <code>enable_ingress</code> to <code>true</code> in the JSON document.</p> <p>To make the ML model's HTTP API accessible, the final step is to create a deployment via the Yatai dashboard. This action generates the necessary Kubernetes resources.</p> <p>Creating a deployment is straightforward using the visual form in the Yatai dashboard. It's generally easier than manually creating the <code>BentoDeployment</code> Kubernetes resources:</p> <p></p> JSON document that describes the Deployment shown in the screenshot<pre><code>{\n    \"cluster_name\": \"default\",\n    \"name\": \"iris\",\n    \"description\": \"\",\n    \"targets\": [\n        {\n            \"type\": \"stable\",\n            \"bento_repository\": \"iris_classifier\",\n            \"bento\": \"mkac3gxctsbd4frz\",\n            \"config\": {\n                \"hpa_conf\": {\n                    \"min_replicas\": 1,\n                    \"max_replicas\": 1\n                },\n                \"resources\": {\n                    \"requests\": {\n                        \"cpu\": \"50m\",\n                        \"memory\": \"500Mi\",\n                        \"gpu\": \"\"\n                    },\n                    \"limits\": {\n                        \"cpu\": \"1000m\",\n                        \"memory\": \"1024Mi\",\n                        \"gpu\": \"\"\n                    }\n                },\n                \"envs\": [],\n                \"runners\": {\n                    \"iris_clf\": {\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"50m\",\n                                \"memory\": \"500Mi\",\n                                \"gpu\": \"\"\n                            },\n                            \"limits\": {\n                                \"cpu\": \"1000m\",\n                                \"memory\": \"1024Mi\",\n                                \"gpu\": \"\"\n                            }\n                        },\n                        \"hpa_conf\": {\n                            \"min_replicas\": 1,\n                            \"max_replicas\": 1\n                        },\n                        \"deployment_strategy\": \"RollingUpdate\"\n                    }\n                },\n                \"enable_ingress\": false,\n                \"deployment_strategy\": \"RollingUpdate\"\n            }\n        }\n    ],\n    \"kube_namespace\": \"yatai\"\n}\n</code></pre>"},{"location":"developer-documentation/diva/","title":"Integration of the DIVA","text":"<p>The Data Integrity and Validation Architecture (DIVA) is one of the main building blocks of the MODERATE platform. It is responsible for the validation and quality analysis of the datasets that are ingested into the platform.</p> <p>This document outlines a possible approach to how the DIVA retrieves datasets from the MODERATE platform to run its data quality pipelines. The main components involved in this workflow include:</p> <ul> <li>The DIVA itself, which retrieves datasets for validation and quality analysis.</li> <li>The object storage service, where the datasets are stored. This service is implemented on top of Google Cloud Storage (GCS), with MODERATE prioritizing the use of the S3-compatible API for GCS to ensure interoperability.</li> <li>The platform HTTP API, serving as the main entry point for DIVA to interact with the MODERATE platform. It exposes the catalogue of datasets, allowing the DIVA to list and retrieve the metadata of the datasets that are available for validation.</li> </ul> <p>The details of the API can be reviewed in its interactive API documentation, which is based on OpenAPI and is automatically generated by FastAPI. You can deploy the API documentation locally by following these instructions:</p> <p>You need Docker to run the development stack.</p> <ol> <li>Clone the repository MODERATE-Project/moderate-platform-api</li> <li>Run the task to deploy the development stack: <code>task dev-up</code></li> <li>Access the API documentation at http://localhost:8000/docs</li> <li>Once you are done, you can stop the development stack by running <code>task dev-down</code></li> </ol> <p>Don't worry if the task complains about not being able to pull the image for the Trust Services. The API documentation will work anyway.</p> <p>The following sequence diagram shows a high-level view of how the DIVA could interact with the MODERATE platform to retrieve datasets and orchestrate the data quality pipelines:</p> <pre><code>sequenceDiagram\n    participant USER as User or Periodic Job\n    participant API\n    participant DIVA\n    participant S3 as Object Storage\n    USER-&gt;&gt;API: Requests validation of dataset\n    alt DIVA is the orchestrator\n        DIVA-&gt;&gt;API: Retrieves list of datasets that are pending validation\n        API-&gt;&gt;DIVA: Responds with the list of datasets\n    else the platform is the orchestrator\n        API-&gt;&gt;DIVA: Requests data validation for a dataset\n        DIVA-&gt;&gt;API: Acknowledges the request\n    end\n    alt access to object storage via the HTTP API\n        DIVA-&gt;&gt;API: Requests download URL for the dataset\n        API-&gt;&gt;DIVA: Responds with pre-signed URL for download \n        S3-&gt;&gt;DIVA: Downloads dataset over HTTP\n    else direct access to object storage\n        DIVA-&gt;&gt;S3: Uses dataset metadata to fetch it directly\n        S3-&gt;&gt;DIVA: Downloads dataset over S3 protocol\n    end\n    DIVA-&gt;&gt;DIVA: Runs the data quality pipeline for the dataset\n    DIVA-&gt;&gt;API: Reports pipeline completion status and results\n    API-&gt;&gt;USER: Checks data quality results </code></pre> <p>Some key points to consider:</p> <ul> <li>The HTTP API endpoints that the DIVA would use to retrieve the list of datasets and report pipeline results are not yet available. They would be implemented if the MODERATE team decides to adopt this approach.</li> <li>There are two possible alternatives for downloading datasets, depending on what is more convenient for the DIVA:<ul> <li>Access to the object storage via the HTTP API, which provides pre-signed URLs for downloading the datasets.</li> <li>Direct access to the object storage service, using the dataset metadata to fetch the datasets directly over the S3 protocol.</li> </ul> </li> <li>There are also two possible alternatives for triggering the data quality pipeline: either DIVA periodically checks the API, or the platform pushes requests to DIVA, which then executes the pipelines on demand.</li> <li>Data quality pipeline runs could be either requested manually by end users via the MODERATE platform web UI or triggered by periodic jobs scheduled to run at specific intervals.</li> </ul>"},{"location":"developer-documentation/diva/#how-to-download-a-dataset-using-the-moderate-api","title":"How to download a dataset using the MODERATE API","text":"<p>This is a specific example of how to implement one of the aforementioned alternatives for downloading datasets, namely the one where access is via the HTTP API, which in turn provides presigned URLs for downloading the dataset files.</p> <p>In this example, we're going to use the public MODERATE API URL, which is deployed at <code>https://api.gw.moderate.cloud</code></p> <p>[!WARNING] Please note that the public deployment of the MODERATE platform is intermittently online for the time being.</p> <p>The first thing that we need to do is get an access token, which is obtained by calling the <code>/api/token</code> endpoint and passing our username and password.</p> <pre><code>$ curl --silent --location 'https://api.gw.moderate.cloud/api/token' --header 'Content-Type: application/x-www-form-urlencoded' --data-urlencode 'username=&lt;username&gt;' --data-urlencode 'password=&lt;password&gt;' | jq\n{\n  \"access_token\": \"&lt;jwt-access-token&gt;\",\n  \"expires_in\": 300,\n  \"refresh_expires_in\": 1800,\n  \"refresh_token\": \"&lt;jwt-refresh-token&gt;\",\n  \"token_type\": \"Bearer\",\n  \"not-before-policy\": 0,\n  \"session_state\": \"&lt;session-uuid&gt;\",\n  \"scope\": \"profile email\"\n}\n</code></pre> <p>In MODERATE, there are two dataset entities:</p> <ul> <li>Asset objects are the actual specific dataset files that users upload to the platform.</li> <li>Assets are logical groupings of asset objects. An asset may have several asset objects. All asset objects in a given asset should have some form of relationship or connection.</li> </ul> <p>For example, an asset could be the energy consumption of a building, and the asset objects in that asset could be dataset files for specific devices within that building.</p> <p>We can browse the catalogue of assets by calling the <code>/asset</code> endpoint. The following request does not apply any filters and limits the results to one, so we will retrieve the first asset:</p> <pre><code>$ curl --silent --location 'https://api.gw.moderate.cloud/asset?limit=1' --header 'Authorization: Bearer &lt;jwt-access-token&gt;' | jq\n[\n  {\n    \"uuid\": \"4477de94-4ffc-490a-ba02-93f0e71db80c\",\n    \"name\": \"One Asset\",\n    \"meta\": null,\n    \"id\": 1,\n    \"objects\": [\n      {\n        \"key\": \"andres.garcia-assets/weather-0558e356-355b-4379-955b-dfe5023da2af.parquet\",\n        \"tags\": null,\n        \"created_at\": \"2024-04-03T10:27:26.247281\",\n        \"series_id\": null,\n        \"sha256_hash\": \"aecc871c3aaac446c60009a2902c2a714a35efd117b440f79f6d9c3856261f8e\",\n        \"proof_id\": null,\n        \"id\": 1\n      },\n      {\n        \"key\": \"andres.garcia-assets/customers-100000-7f596126-4771-45e1-9e8a-237e3635eb7c.csv\",\n        \"tags\": null,\n        \"created_at\": \"2024-04-03T10:33:05.948939\",\n        \"series_id\": null,\n        \"sha256_hash\": \"446d645458479d841c8c0239f6d4f882e4735e63db21ff980c53058eabc6beda\",\n        \"proof_id\": null,\n        \"id\": 4\n      },\n      {\n        \"key\": \"andres.garcia-assets/flights-1m-48653f22-0d5c-4f10-aa2b-81ef35959445.parquet\",\n        \"tags\": null,\n        \"created_at\": \"2024-04-03T10:33:20.951760\",\n        \"series_id\": null,\n        \"sha256_hash\": \"71ccd0758a73ac9d89ccda6107e3cbfc7e4cd3249d3766f881e48f7513e601fd\",\n        \"proof_id\": null,\n        \"id\": 5\n      }\n    ],\n    \"access_level\": \"public\"\n  }\n]\n</code></pre> <p>Now that we know the ID of the asset that we want to download, we can call the <code>/asset/&lt;id&gt;/download-urls</code> endpoint, which will return a list of presigned download URLs. These URLs enable any user (e.g., a software service in the DIVA) to download the asset object files in a time-limited fashion by embedding the credentials into the download URL itself.</p> <pre><code>$ curl --silent --location 'https://api.gw.moderate.cloud/asset/1/download-urls' --header 'Authorization: Bearer &lt;jwt-access-token&gt;' | jq\n[\n  {\n    \"key\": \"andres.garcia-assets/weather-0558e356-355b-4379-955b-dfe5023da2af.parquet\",\n    \"download_url\": \"https://storage.googleapis.com/moderate-platformapi/andres.garcia-assets/weather-0558e356-&lt;...&gt;\"\n  },\n  {\n    \"key\": \"andres.garcia-assets/customers-100000-7f596126-4771-45e1-9e8a-237e3635eb7c.csv\",\n    \"download_url\": \"https://storage.googleapis.com/moderate-platformapi/andres.garcia-assets/customers-100000-7f596126-&lt;...&gt;\"\n  },\n  {\n    \"key\": \"andres.garcia-assets/flights-1m-48653f22-0d5c-4f10-aa2b-81ef35959445.parquet\",\n    \"download_url\": \"https://storage.googleapis.com/moderate-platformapi/andres.garcia-assets/flights-1m-48653f22-&lt;...&gt;\"\n  }\n]\n</code></pre>"},{"location":"developer-documentation/glossary/","title":"Glossary","text":"<p>This page serves as a reference for a collection of terms utilized in the deliverables and technical documentation. Its purpose is to establish a shared understanding, ensuring that terms with ambiguities are employed accurately.</p> Term Definition Data owner Owner of an datasets or services Decentralized Identifiers Decentralized Identifiers (DIDs) epresent a novel form of URIs. They serve to reference a subject and facilitate trustworthy interactions related to that subject. To obtain additional information, please refer to the DID standard. https://www.w3.org/TR/did-core/ Metadata Information regarding datasets or services, delineating their content, context, data format and associated rights. MODERATE Platform The platform encompasses all cloud services and applications generated by the MODERATE project. Essentially, it is the unified entity that enables the collaboration of the various outputs of the project, including ML models, web applications, and supporting services like Identity and Access Management. Storage &amp; Analytics Layer The subset of services within the platform that facilitate the storage of diverse data assets and their efficient processing. These services include, for example, relational databases, object-storage services and workflow orchestration tools. Analytics The examination of large datasets using mathematical methods (e.g. statistical methods) and/or machine learning approaches to devise meaningful models of building-related processes. Tools The software means (i.e. both the front and back-ends) to allow a user to interact with the models and request to, and obtain services from the MODERATE platform. Services The ultimate scope of MODERATE platform, to provide users insight into the building-related processes and let them gain new knowledge and decision-making capacity. Trust proof Cryptographic proof employed to verify the authenticity and integrity of an asset and its metadata. This proof is stored on the Tangle to ensure immutability."},{"location":"developer-documentation/integration/","title":"Integrating New Services and Applications","text":"<p>This page provides a guide on how to integrate a new service or application into the platform. It is mainly aimed at developers who are working on a tool or web application and are wondering how they can go from their local development environment to having their services deployed in the Kubernetes cluster within the platform's cloud services.</p> <p>In this integration process, there are two distinct actors:</p> <ul> <li>The developer of the web application or service: This developer possesses a deep understanding of their own code and has a clear vision of its requirements in terms of services, databases, and other components. However, they may not be familiar with the specific workings of the platform or the configuration of the Kubernetes cluster.</li> <li>The administrator of the platform: This actor perceives the application provided by the developer as a \"black box\" and possesses the knowledge required to deploy this application in a way that enables it to function alongside existing services (such as databases) and be accessible to end users of the platform.</li> </ul> <p>The contact email for the platform administrator can be found in the MODERATE Github organization.</p>"},{"location":"developer-documentation/integration/#steps-for-integration","title":"Steps for Integration","text":"<p>The following diagram shows the steps of the development and integration process:</p> <p></p>"},{"location":"developer-documentation/integration/#step-1-development","title":"\ud83d\udc69\u200d\ud83d\udcbb Step 1: Development","text":"<p>Who: Developer</p> <p>Development of the web application or service using whatever technology stack the developer prefers.</p> <p>The credentials and configuration parameters for external services, such as databases, message queues, and object storage services, should be expected by the application as environment variables. These variables should be clearly documented in the repository's README file and would serve as the basis for the configuration of the application in the platform.</p> <p>About configuration parameters</p> <p>It is crucial to avoid hardcoding configuration parameters, such as database credentials and connection URIs, directly in the application or relying on manual updates in a configuration file once the application or service is deployed. Please use environment variables instead.</p> <p>Although it is fine to start in a private repository, development should eventually be centralized in a repository within MODERATE's GitHub organization.</p>"},{"location":"developer-documentation/integration/#step-2-dockerfile","title":"\ud83d\udce6 Step 2: Dockerfile","text":"<p>Who: Developer</p> <p>Write a Dockerfile and test that the containers work properly in a local environment.</p> <p>For example, this is the Dockerfile used to build the <code>moderate-docs</code> image, which is the documentation website you are currently reading.</p> <p>This Dockerfile should be contained in the repository of the application or service. It should be located in the root directory of the repository and named <code>Dockerfile</code>.</p>"},{"location":"developer-documentation/integration/#step-3-continuous-integration","title":"\ud83d\udc77\u200d\u2642\ufe0f Step 3: Continuous Integration","text":"<p>Who: Developer or Administrator</p> <p>Create an Action in the repository for the images to be built on each push to the <code>main</code> branch and then uploaded to MODERATE's image registry. Actions need to be located in a YAML file in the <code>.github/workflows</code> directory of the repository.</p> <p>For example, the following is the Action workflow configuration file for the MODERATE HTTP API. Please note the following details:</p> <ul> <li>The workflow configuration file for your own application should be mostly the same. The only parameter that should change is the <code>image_name</code>, which is the name of the image in the image registry. This name needs to be unique across the entire MODERATE platform.</li> <li>We simply reuse an existing configuration file that is already present in the <code>moderate-docs</code> repository.</li> <li>All the variables and secrets (e.g. <code>secrets.WIF_PROVIDER</code>) have already been configured at the organization level by the administrator. Note that the administrator needs to manually enable the secrets for a particular repository before they are available.</li> </ul> Example of a workflow file to build and push an image to MODERATE's image registry<pre><code>name: Build and push an image to Google Artifact Registry (GAR)\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  call-build-push-artifact-registry:\n    uses: MODERATE-Project/moderate-docs/.github/workflows/reusable-build-push-gar.yml@main\n    with:\n      project_id: ${{ vars.DEFAULT_GAR_PROJECT_ID }}\n      gar_location: ${{ vars.DEFAULT_GAR_LOCATION }}\n      gar_repo: ${{ vars.DEFAULT_GAR_REPOSITORY }}\n      image_name: moderate-api\n    secrets:\n      wif_provider: ${{ secrets.WIF_PROVIDER }}\n      wif_service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }}\n</code></pre>"},{"location":"developer-documentation/integration/#step-4-terraform-resources","title":"\ud83c\udfd7\ufe0f Step 4: Terraform Resources","text":"<p>Who: Administrator</p> <p>Create the Terraform resources that define the Kubernetes resources, which, in turn, represent the deployment of the web application or service.</p> <p>Once the application is defined as Terraform resources within the moderate-infrastructure repository, it can seamlessly integrate into the platform's life cycle. This enables deployment, destruction, and re-creation with minimal effort.</p>"},{"location":"developer-documentation/integration/#step-5-deployment","title":"\u2601\ufe0f Step 5: Deployment","text":"<p>Who: Administrator</p> <p>Deploy these Terraform resources to MODERATE's cloud platform.</p>"},{"location":"developer-documentation/integration/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"developer-documentation/integration/#am-i-restricted-in-the-technology-stack-that-i-can-use","title":"Am I restricted in the technology stack that I can use?","text":"<p>No. You can use any technology stack that you want. However, you should be aware that the administrator won't know the internal details of your application, and will thus be unable to help you with issues outside of integration with the platform and deployment.</p>"},{"location":"developer-documentation/integration/#are-there-any-examples-of-applications-or-services-that-have-already-been-integrated-into-the-platform","title":"Are there any examples of applications or services that have already been integrated into the platform?","text":"<p>Sure! Take a look at the <code>moderate-platform-api</code> repository.</p>"},{"location":"developer-documentation/integration/#what-should-i-do-to-deploy-the-databases-or-other-services-that-my-application-depends-on","title":"What should I do to deploy the databases or other services that my application depends on?","text":"<p>The administrator will take care of deploying the databases and other services that your application depends on. You should simply document the configuration parameters that your application expects to find in the environment variables.</p> <p>You can get in touch with the administrator to discuss the requirements of your application and the best way to integrate it into the platform.</p>"},{"location":"developer-documentation/integration/#do-i-have-to-do-something-specific-for-my-container-to-run-on-kubernetes","title":"Do I have to do something specific for my container to run on Kubernetes?","text":"<p>No. The containers that you build with your Dockerfile will run on Kubernetes without any modifications. The administrator will take care of creating the Kubernetes resources that will run your containers.</p>"},{"location":"developer-documentation/integration/#will-new-versions-of-my-application-be-automatically-deployed","title":"Will new versions of my application be automatically deployed?","text":"<p>No for the time being. The administrator will take care of manually deploying new versions of your application. However, we will probably move to a Continuous Deployment model in the future.</p>"},{"location":"developer-documentation/integration/#my-application-is-offline-what-should-i-do","title":"My application is offline. What should I do?","text":"<p>Please note that the cloud platform will be intermittently unavailable during the development phase. If you need to test your application for a continued period of time, please contact the administrator to ensure that the platform is available.</p>"},{"location":"tools-and-services/","title":"Tools and services","text":"<p>Availability of public tools and services</p> <p>MODERATE is under active development, so the infrastructure may occasionally be offline for maintenance, cost optimization, or other development purposes. If any links are not working, please either wait for redeployment or contact us via email for access.</p>"},{"location":"tools-and-services/#building-benchmarking","title":"\ud83d\udcca Building Benchmarking","text":"<p>Benchmarking is a web tool that allows the comparison of building performances. It provides a comprehensive view of energy efficiency and performance metrics for a selected building, including the most relevant KPIs and their interdependencies.</p> <ul> <li> Git repository</li> <li> Documentation</li> <li> Public application (development environment)</li> <li> Public application</li> </ul>"},{"location":"tools-and-services/#brickllm","title":"\ud83e\udde0 BrickLLM","text":"<p>BrickLLM is a web application that generates RDF files conforming to the BrickSchema ontology by leveraging Large Language Models (LLMs). The application is built on top of the Python library BrickLLM.</p> <ul> <li> Git repository</li> <li> Public application</li> </ul>"},{"location":"tools-and-services/#contextual-anomaly-detector","title":"\ud83d\udcc8 Contextual Anomaly Detector","text":"<p>The Contextual Anomaly Detector identifies anomalies in meter-level energy consumption using supervised and unsupervised analytics combined with the distance-based contextual matrix profile (CMP) algorithm. It autonomously detects infrequent patterns in energy timeseries, applies boundary conditions, and ranks anomalies by severity based on load shape and magnitude. With self-tuning capabilities, the tool helps energy and facility managers detect abnormal energy usage and inefficiencies, enabling informed decisions to reduce waste and optimize performance.</p> <ul> <li> Git repository</li> <li> Documentation</li> </ul>"},{"location":"tools-and-services/#encome","title":"\ud83c\udfe0 Encome","text":"<p>The ENergy COnservation TOol - ENCOME - is a comprehensive assessment platform that evaluates building energy performance improvements through Energy Conservation Measures (ECMs) using advanced EN ISO 52016/52010 standards</p> <ul> <li> Documentation</li> <li> Public application</li> </ul>"},{"location":"tools-and-services/#geoclustering-tool","title":"\ud83c\udfd7\ufe0f Geoclustering Tool","text":"<p>The Geoclustering Tool is a web application that allows users to cluster buildings based on their geospatial location and other relevant features. The tool is built using Python and is designed to be easy to use and understand.  The actual versione analyze the Energy PErformance Certificate of the Pedmont region, Italy. A sensitivity analysis is also available to evaluate the impact of different clustering parameters on a selected target.</p> <ul> <li> Git repository</li> <li> Documentation</li> <li> Public application (development environment) </li> <li> Public application</li> </ul>"},{"location":"tools-and-services/#lec-assessment-tool","title":"\ud83c\udfd8\ufe0f LEC Assessment Tool","text":"<p>Local Energy Communities (LECs) are pivotal in advancing building decarbonization, fostering social cohesion, and promoting the integration of renewable energy sources. This tool streamlines the establishment of LECs by pinpointing optimal locations for their formation, enabling stakeholders to efficiently identify viable LEC sites.</p> <ul> <li> Git repository</li> <li> Public application</li> <li> Documentation</li> </ul>"},{"location":"tools-and-services/#measurement-and-verification-tool","title":"\ud83d\udcb0 Measurement and Verification Tool","text":"<p>The Measurement and Verification Tool is a web application that allows users to perform measurement and verification of building energy performance, using the option C of the International Performance Measurement and Verification Protocol (IPMVP).</p> <ul> <li> Git repository</li> <li> Documentation</li> </ul>"},{"location":"tools-and-services/#solar-cadastre","title":"\ud83c\udf1e Solar Cadastre","text":"<p>The Solar Cadastre (SC) enables users to evaluate the solar energy potential of buildings and assess the efficiency of installed solar panels. Through an interactive map, users can select buildings, perform energy calculations, and retrieve cadastral data to support decision-making.</p> <ul> <li> Git repository</li> <li> Documentation</li> <li> Public application</li> </ul>"},{"location":"tools-and-services/#fault-detection-and-forecasting","title":"\ud83d\udd0d Fault Detection and Forecasting","text":"<p>The Fault Detection and Forecasting tool offers a customizable environment for setting up time series data analytics to detect anomalies and forecast system behavior. Through the SYNAVISION platform, it provides intelligent monitoring that enables predictive maintenance for technical building systems, helping energy managers identify faults in near-real time and optimize operations.</p> <ul> <li> Documentation</li> </ul>"},{"location":"tools-and-services/#energy-system-optimization","title":"\u26a1 Energy System Optimization","text":"<p>The Energy System Optimization tool offers a customizable environment for optimizing building energy systems through time series data analytics. Through the SYNAVISION platform, it enables smart control of heating requirements based on weather forecasts, allowing for automated temperature adjustments that maximize comfort while minimizing energy consumption. The tool is especially effective when integrated with MyGekko building automation systems.</p> <ul> <li> Documentation</li> </ul>"},{"location":"tools-and-services/#timeseries-based-energy-benchmarking","title":"\ud83d\udcc9 Timeseries-based Energy Benchmarking","text":"<p>The Timeseries-based Energy Benchmarking tool performs advanced benchmarking of buildings' operational energy performance using hourly electricity consumption time series. It evaluates and compares a building's electricity consumption against a reference peer group selected from a larger stock of buildings (more than 700) based on similar consumption features. The tool incorporates preprocessing, peer identification, KPI calculation, and benchmarking to deliver actionable insights for building energy managers.</p> <ul> <li> Git repository</li> <li> Documentation</li> <li> Public application</li> </ul>"},{"location":"tools-and-services/#quality-check-report-tool","title":"\u2705 Quality Check Report Tool","text":"<p>The Quality Check Report Tool is a web-based application designed to validate and analyze Energy Performance Certificates (EPC) in XML format. This tool automates the process of identifying inconsistencies and discrepancies in EPCs by applying a set of predefined validation rules. The application provides immediate feedback through a user-friendly interface, helping users ensure their certificates meet all required standards and specifications.</p> <ul> <li> Git repository</li> <li> Documentation</li> <li> Public application</li> </ul>"},{"location":"tools-and-services/benchmarking/","title":"Benchmarking Tool","text":"<p>Abstract</p> <p>The benchmarking tool is designed to visualize, compare, and analyze the performance of buildings using specific Key Performance Indicators (KPIs). This tool evaluates various types of data, from tabular data like Energy Performance Certificates to time-series data directly monitored in the building, considering not only energy consumption but also comfort conditions. With correct data normalization, an energy manager can assess building trends using simple graphs and identify potential improvement actions, reducing consumption and/or enhancing comfort conditions. Additionally, the tool can provide insights into energy efficiency measures and their impact on operational costs, allowing for more informed decision-making and strategic planning. </p>"},{"location":"tools-and-services/benchmarking/#introduction","title":"Introduction","text":"<p>In an era where energy efficiency and sustainability are key priorities, understanding how a building performs compared to industry benchmarks is essential. The Building Benchmarking Tool is a web-based platform that enables users to analyze and compare the energy performance of buildings.</p> <p>By leveraging a comprehensive database of reference buildings, the tool provides insights into energy consumption, indoor comfort, and efficiency levels, helping professionals make data-driven decisions. Through interactive visualizations and customizable inputs, users can assess their building's energy footprint, identify potential inefficiencies, and explore strategies for optimization. Additionally, the tool considers climate zone variations, ensuring that benchmarks are accurate and region-specific.</p> <p>Whether you are an energy manager, energy consultant, or building owner, this tool serves as a valuable resource for improving sustainability and reducing operational costs. A new section has been integrated into the tool that includes the evaluation of possible sensor malfunctions or anomalies. This part represents a preliminary assessment of the integration of moderate applications such as fault detection.</p> Benchmarking multiple buildings"},{"location":"tools-and-services/benchmarking/#user-guide","title":"User Guide","text":"<p>The tool is composed of five main sections, accessible through the navbar in the header. These sections include four main categories:</p>"},{"location":"tools-and-services/benchmarking/#building","title":"Building","text":"<p>Users can view all managed buildings, including the latest sensor readings and the ability to view comfort performances and anomalies for a selected building.</p> List of all buildings"},{"location":"tools-and-services/benchmarking/#processing","title":"Processing","text":"<p>In this section, users can connect processed and cleaned time-series data for a specific building sensor from a CSV format to the database where the measured data is stored. Moreover, using the Python library Brickllm, users can update building metadata. These metadata are saved in RDF format and generated following the brickschema ontology. The Brickllm library allows the regeneration of the building description file, updating, for example, the identifier value for the new connected sensor. For its use, an API key for a specific LLM provider (ChatGPT, Fireworks, or Anthropic) is required.</p>"},{"location":"tools-and-services/benchmarking/#benchmarking","title":"Benchmarking","text":"<p>In this section, users can visualize the performances of all buildings through specific KPIs. Additionally, users can compare these buildings with each other or select two for comparison.</p> Comparison of buildings performances"},{"location":"tools-and-services/benchmarking/#analysis","title":"Analysis","text":"<p>This section allows data analysis according to three main subsections:</p> <ul> <li>Comfort: Users can view internal temperature trends, assess periods of overheating and overcooling, and evaluate a regression model between internal and external temperatures. This assessment helps identify potential issues related to the building's system or envelope.</li> </ul> Comfort analysis <ul> <li>Energy: Users can evaluate energy consumption performances, even linking to an average energy cost per kWh defined by the user. In this section, typical profiles during workdays, weekends, or weekly profiles can also be viewed.</li> </ul> Monthly energy cost  Regression analysis energy vs Degree Days  <ul> <li>Anomalies: Possible anomalies in the dataset for different sensors are identified. Each anomaly can be directly visualized in a graph.</li> </ul> Identified anomalies"},{"location":"tools-and-services/benchmarking/#references","title":"References","text":"<ul> <li>Benchmarking tool</li> </ul>"},{"location":"tools-and-services/benchmarking_timeseries/","title":"Timeseries-based Energy Benchmarking","text":"<p>Abstract</p> <p>The \"Timeseries-based Energy Benchmarking\" tool performs advanced benchmarking of buildings' operational energy performance using hourly electricity consumption time series. It evaluates and compares a building's electricity consumption against a reference peer group selected from a larger stock of buildings (more than 700) based on similar consumption features. The tool incorporates preprocessing, peer identification, KPI calculation, and benchmarking to deliver actionable insights for building energy managers.</p> Screenshot of the Timeseries Energy Benchmarking application interface."},{"location":"tools-and-services/benchmarking_timeseries/#introduction","title":"Introduction","text":"<p>Accurate energy benchmarking is a cornerstone of effective building energy management. By comparing a building's electricity consumption to that of similar buildings, facility managers can identify opportunities for operational improvements.  The \"Timeseries-based Energy Benchmarking tool\" leverages hourly electrical consumption time series data alongside outside air temperature and few high-level building characteristics to provide a comprehensive evaluation of building energy performance relative to peer buildings.</p> <p>The tool's workflow includes preprocessing raw data to ensure quality, identifying a peer group based on load characteristics, calculating key performance indicators (KPIs), and benchmarking each KPI to provide performance scores.  This facilitates a clear understanding of operational strengths and weaknesses, guiding energy management strategies.</p>"},{"location":"tools-and-services/benchmarking_timeseries/#workflow-overview","title":"Workflow Overview","text":"<p>The benchmarking workflow consists of the following key processes:</p>"},{"location":"tools-and-services/benchmarking_timeseries/#preprocessing-of-time-series-data","title":"Preprocessing of time series data","text":"<p>Raw electricity consumption data is cleaned using statistical outlier detection and decomposed with MSTL (Multiple Seasonal-Trend decomposition with Loess) to remove inconsistencies and prepare the data for analysis.</p>"},{"location":"tools-and-services/benchmarking_timeseries/#peer-identification","title":"Peer identification","text":"<p>Time series features are extracted for each building, including thermal dependency, reference load conditions (weekday/weekend, seasonal groups), load-shape factors, and mean consumption. These features enable selection of a peer group exhibiting similar load behaviors.</p>"},{"location":"tools-and-services/benchmarking_timeseries/#key-performance-indicators-kpis-calculation","title":"Key Performance Indicators (KPIs) calculation","text":"<p>The tool computes KPIs describing various energy performance aspects:</p> <ul> <li> <p>Energy Use Intensity</p> </li> <li> <p>Operational Schedule Efficiency: evaluate the consumption ratio     between working hours, non-working hours, and weekends.</p> </li> <li> <p>Load Volatility: evaluates the variability in daily load profiles.</p> </li> <li> <p>Anomaly Detection: Identification of atypical consumption patterns.</p> </li> <li> <p>Load Pattern Frequency: Analysis of the variety and recurrence of     load shapes within a specific load condition.</p> </li> <li> <p>Benchmarking: Each KPI is benchmarked against the selected set of     peers. For each load condition, a performance score for each KPI     ranging from 0 to 100 is computed, where 100 indicates best-in-class     performance. These scores provide building managers and energy     analysts with a clear understanding of how a building performs in each     area and where there are opportunities for improvement.</p> </li> </ul> Summary visualization of benchmarking results including KPI scores."},{"location":"tools-and-services/benchmarking_timeseries/#input-data","title":"Input data","text":"<p>The tool requires the following input data:</p> <ul> <li>Hourly electrical energy consumption timeseries: Provided as a CSV file containing hourly electricity consumption data.</li> <li>Outside air temperature timeseries: Provided as a CSV file containing hourly outside air temperature data.</li> <li>End-use category of the building: A string indicating the building's primary usage (e.g., Office, Educational).</li> <li>Floor area of the building: Numeric value representing the building's floor area in square meters.</li> <li>[Optional] State: A string specifying the building's state, used to retrieve the local holiday calendar for enhanced analysis (only for US buildings).</li> </ul> <p>If the required data is unavailable, the tool provides example buildings to explore benchmarking functionality. Users can toggle usage modes via the interface control.</p>"},{"location":"tools-and-services/benchmarking_timeseries/#usage","title":"Usage","text":"<p>The application comes with an interface that allows users to upload the required data files. The interface contains the description of the input data format and the expected columns. The user can upload the files and enter the remaining information.</p> Screenshot of the Timeseries Energy Benchmarking application interface. <p>Once the data is uploaded, the user can start the benchmarking process by clicking the \"Perform analysis\" button. Then, the user can browse the results in the other tabs of the interface. Each tab contains a description of the results presented.</p> Peer identification tab. <p>Known limitations and issues</p> <p>A known limitation is the static nature of the tool. Future deployment will improve the dynamic integration of new buildings in order to update the stock considered during the peer identification process. Furthermore, the interoperability of the tool will be improved through the use of ontology-based metadata models.</p>"},{"location":"tools-and-services/benchmarking_timeseries/#references","title":"References","text":"<p>You can cite this work by using the following plain text citation</p> <p>M.S. Piscitelli, R. Giudice, A. Capozzoli, A holistic time series-based energy benchmarking framework for applications in large stocks of buildings, Applied Energy, 2024, https://doi.org/10.1016/j.apenergy.2023.122550.</p> <p>Other references</p> <ul> <li>Published Paper</li> <li>Original repository</li> <li>BAEDA LAB</li> </ul>"},{"location":"tools-and-services/cmp/","title":"Contextual Anomaly Detector Tool","text":"<p>Abstract</p> <p>The \"Contextual anomaly detector tool\" is designed to identify anomalies in energy consumption at the meter level, leveraging both supervised and unsupervised analytics techniques along with the distance-based contextual matrix profile (CMP) algorithm. This tool autonomously detects infrequent subsequences in energy consumption timeseries, taking into account specific boundary conditions, and ranks anomalies based on a severity score that reflects the shape and magnitude of electrical load patterns. With self-tuning capabilities, it enables energy and facility managers to quickly recognize abnormal and non-optimal energy performance patterns, thereby supporting better decision-making to reduce inefficiencies and energy waste. </p>"},{"location":"tools-and-services/cmp/#introduction","title":"Introduction","text":"<p>Recently, the spread of IoT technologies has led to an unprecedented acquisition of energy-related data providing accessible knowledge of the actual performance of buildings during their operation. A proper analysis of such data supports energy and facility managers in spotting valuable energy saving opportunities. In this context, anomaly detection and diagnosis (ADD) tools allow a prompt and automatic recognition of abnormal and non-optimal energy performance patterns enabling a better decision-making to reduce energy wastes and system inefficiencies.</p> <p>In this context, the \"Contextual anomaly detector tool\" was developed to identify energy consumption anomalies at meter-level. The process leverages supervised and unsupervised analytics techniques coupled with the distance-based contextual matrix profile (CMP) algorithm to discover infrequent subsequences in energy consumption timeseries considering specific boundary conditions. The proposed process has self-tuning capabilities and can rank anomalies according to severity score calculated with reference to shape and magnitude of electrical load subsequences.</p> <p>The tool has been employed in a real-world scenario to detect anomalies in the energy consumption of a university campus, and it is a valuable tool for energy and facility managers to spot unexpected behaviour, producing an interactive report as output.</p> Result of daily load profiles anomaly detection"},{"location":"tools-and-services/cmp/#usage","title":"Usage","text":"<p>The tool comes with a CLI that helps you to execute the script with the desired commands</p> <pre><code>$ python -m src.cmp.main -h\n\nMatrix profile\n\npositional arguments:\n  input_file     Path to file\n  variable_name  Variable name\n  output_file    Path to the output file\n\noptions:\n  -h, --help     show this help message and exit\n</code></pre> <p>The arguments to pass to the script are the following:</p> <ul> <li><code>input_file</code>: The input dataset via an HTTP URL. The tool should then download the dataset from that URL; since it's a   pre-signed URL, the tool would not need to deal with authentication\u2014it can just download the dataset directly.</li> <li><code>variable_name</code>: The variable name to be used for the analysis (i.e., the column of the csv that contains the   electrical load under analysis).</li> <li><code>output_file</code>: The local path to the output HTML report. The platform would then get that HTML report and upload it to   the object   storage service for the user to review later.</li> </ul> <p>You can run the main script through the console using either local files or download data from an external url. This repository comes with a sample dataset that you can use to generate a report and you can pass the local path as <code>input_file</code> argument.</p>"},{"location":"tools-and-services/cmp/#data-format","title":"Data format","text":"<p>The tool requires the user to provide a csv file as input that contains electrical power timeseries for a specific building, meter or energy system (e.g., whole building electrical power timeseries). The <code>csv</code> is a wide table format as follows:</p> <pre><code>timestamp,column_1,temp\n2019-01-01 00:00:00,116.4,-0.6\n2019-01-01 00:15:00,125.6,-0.9\n2019-01-01 00:30:00,119.2,-1.2\n</code></pre> <p>The csv must have the following columns:</p> <ul> <li><code>timestamp</code> [case sensitive]: The timestamp of the observation in the format <code>YYYY-MM-DD HH:MM:SS</code>. This column is   supposed to be in   UTC timezone string format. It will be internally transformed by the tool into the index of the dataframe.</li> <li><code>temp</code> [case sensitive]: Contains the external air temperature in Celsius degrees. This column is required to perform   thermal sensitive   analysis on the electrical load.</li> <li><code>column_1</code>: Then the dataframe may have <code>N</code> arbitrary columns that refers to electrical load time series. The user has   to specify the column name that refers to the electrical load time series in the <code>variable_name</code> argument.</li> </ul>"},{"location":"tools-and-services/cmp/#run","title":"Run","text":"<p>Now you can run the script from the console by passing the desired arguments. In the following we pass the sample dataset <code>data.csv</code> as input file and the variable <code>Total_Power</code> as the variable name to be used for the analysis. The output file will be saved in the <code>results</code> folder.</p> <pre><code>$ python -m src.cmp.main src/cmp/data/data.csv Total_Power src/cmp/results/reports/report.html\n\n2024-08-13 12:45:42,821 [INFO](src.cmp.utils) \u2b07\ufe0f Downloading file from &lt;src/cmp/data/data.csv&gt;\n2024-08-13 12:45:43,070 [INFO](src.cmp.utils) \ud83d\udcca Data processed successfully\n\n*********************\nCONTEXT 1 : Subsequences of 05:45 h (m = 23) that start in [00:00,01:00) (ctx_from00_00_to01_00_m05_45)\n99.997%        0.0 sec\n\n- Cluster 1 (1.660 s)   -&gt; 1 anomalies\n- Cluster 2 (0.372 s)   -&gt; 3 anomalies\n- Cluster 3 (0.389 s)   -&gt; 4 anomalies\n- Cluster 4 (0.593 s)   -&gt; 5 anomalies\n- Cluster 5 (-)         -&gt; no anomalies green\n\n[...]\n\n2024-08-13 12:46:27,187 [INFO](__main__) TOTAL 0 min 44 s\n2024-08-13 12:46:32,349 [INFO](src.cmp.utils) \ud83c\udf89 Report generated successfully on src/cmp/results/reports/report.html\n</code></pre> <p>At the end of the execution you can download the report ad HTML file format.</p>"},{"location":"tools-and-services/cmp/#result-structure-and-interpretation","title":"Result structure and interpretation","text":"<p>The results were organized in the final report into different sections as shown in Figure below. The report section (a) was conceived as a summary of the results of the overall analysis. In this part, the main characteristics of the input time series are calculated and displayed in a bullet point (e.g., number of observations) and the entire time series is represented in an interactive line plot where the anomalous sub-sequences identified by the algorithm are highlighted in a red scale according to the severity level, ranging in the interval from 6 to 8. The line plot can be navigated by the user and it is possible to zoom onto a specific area of the time-series allowing the detailed exploration of the load profiles.</p> Structure of the interactive report resulting from the contextual anomaly detector tool. <p>As the report progresses, section (b) presents the cluster analysis results, highlighting the grouping of daily load profiles and identifying time windows along with their context definitions. Next, the analysis results are shown by context and group. For each context, the overall contextual matrix profile is displayed, accompanied by a brief context overview (c). Then, for each group, the group-specific contextual matrix profile is provided, along with a representation of the group's daily load profiles (d). On the left, both the context and sub-daily time window are visualized in dark orange and light orange, respectively. The daily load profiles for the group are shown in gray, while those identified as anomalous for the specific context and time window are highlighted in red. This visual representation is supported by a table listing the dates of the identified anomalies and their corresponding anomaly scores.</p> <p>Known limitations and issues</p> <p>A known limitation is the static generation of clusters and time windows that will be generalized and will be performed automatically based on the data provided. Moreover the delivery of the informative output will be enhanced.</p>"},{"location":"tools-and-services/cmp/#references","title":"References","text":"<p>You can cite this work by using the following plain text citation</p> <p>Chiosa, Roberto, et al. \"Towards a self-tuned data analytics-based process for an automatic context-aware detection and diagnosis of anomalies in building energy consumption timeseries.\" Energy and Buildings 270 (2022): 112302.</p> <p>Other references</p> <ul> <li>Published Paper</li> <li>Contextual Anomaly Detector repository</li> <li>Series Distance Matrix repository</li> <li>Stumpy Package</li> </ul>"},{"location":"tools-and-services/ecm/","title":"ENCOME - ENergy COnservation MEasure tool","text":"<p>Abstract</p> <p>The Energy Conservation Tool is a comprehensive assessment platform that evaluates building energy performance improvements through Energy Conservation Measures (ECMs) using advanced EN ISO 52016/52010 standards. Built on the open-source pyBuildingEnergy library developed with HE Moderate, BuildON and Infinite projects, it provides semi-dynamic energy calculations with hourly resolution, surpassing traditional static methods used in energy certificates. The tool enables assessment of both building envelope measures (insulation, windows) and system improvements (HVAC, photovoltaics) through integration with the 2050 Materials database of market-available components. By transitioning from static to dynamic hourly calculations, it delivers unprecedented accuracy for evidence-based energy renovation planning. This comprehensive approach supports building professionals in making informed decisions for sustainable building transformations and strategic renovation investments</p>"},{"location":"tools-and-services/ecm/#introduction","title":"Introduction","text":"<p>ENCOME aims to assess improvements in the building's energy performance when energy improvement measures are implemented.  The tool starts with an evaluation of the building\u2019s baseline through an energy simulation based on EN ISO 52016 and 52010 standards.  These standards, referenced for energy performance assessment in the revised EPBD, represent an improvement compared to the calculations applied in energy certificates. In fact, it shifts from a static calculation to a semi-dynamic calculation, where energy performance is evaluated not only on an annual and monthly basis but also on a daily and hourly basis. In this context, in collaboration with the HE BuildOn and H2020 Infinite and Moderate projects, the first open-source software library (pyBuildingEnergy) available to the community has been developed, enabling the calculation of the building\u2019s energy demand for heating and cooling using EN ISO 52016 and 52010. Additionally, the calculation of domestic hot water has been added to provide a more detailed and comprehensive assessment, up to on hourly basis. This library serves as the calculation engine for the tool, through which various assessments are carried out, both initial (baseline) and virtually applying new ECMs (Energy Conservation Measures)to the building allowing for better planning of real ECM developments for that building. The applicable ECMs pertain not only to the building envelope but also to the building systems. For the envelope, it is possible to assess new opaque components by defining layers that characterize walls, floors, and roofs, while for the transparent part, new window components can be defined. For the systems, it is possible to define windows with improved performance. To achieve this, a database based on market-available construction materials, components and systems can be accessed by collaboration with the company 2050 Materials, or entered directly by the user. The ECMs that can be implemented also target the deployment of renewable energy sources. In fact, in a dedicated section, it is possible to simulate the performance of the building if a photovoltaic system were to be installed. To do this, it is assumed that the building\u2019s thermal load is generated by an electric generator or heat pump, with the energy demand partially met by the photovoltaic system.</p> <p>The tool is based on the  following schema:</p> General schema of the ECM tool, including modular components localized for Italy, and interoperability with other MODERATE tools and models"},{"location":"tools-and-services/ecm/#user-guide","title":"User Guide","text":""},{"location":"tools-and-services/ecm/#input","title":"INPUT","text":"<p>To simulate the tool some inputs are required (similar to the ones required for the EPC certification):</p> <ul> <li> <p>Building location:     Weather information, such as external temperature and solar radiation, useful for simulating the building, is directly taken from the PVGIS, using the dedicated API and providing the latitude and longitude of the building as input. In the tool, this information can be provided either by directly selecting the building on the map or by typing the building's address in the dedicated space </p> <p> Geolocation of the building  - Building information: where the information provided during the building creation phase is displayed, such as building name, year of construction, type, project name, and simulation number </p> <p> Geolocation of the building </p> </li> <li> <p>Building geometry:     in this field, the geometric information of the building is provided, such as perimeter, area, volume, etc</p> <p> Building geometry </p> </li> <li> <p>Building envelope:     here the user is asked to enter information related to the envelope, both for the opaque and transparent components</p> <p> List of facade elements (opaque and transparent) </p> <p> Inputs to create a facade element </p> </li> <li> <p>Building systems:     represent simplified information about the heating and cooling system. The user can provide either the efficiency of the different subsystems for the heating system or specify certain components such as radiators, radiant panels, or others for the emission system</p> <p> Heating and Cooling systems inputs </p> </li> <li> <p>Ventilation:     hourly air changes are required, as well as any extra air changes during the building's occupancy periods </p> <p> Ventilation </p> </li> <li> <p>Internal Gains:     these are internal gains that can vary in case of building occupancy, in which case a specific value must be provided </p> <p> Internal Gains </p> </li> <li> <p>Thermal bridges:     value of the heat transmission coefficient due to thermal bridge</p> <p> Thermal Bridges input </p> </li> <li> <p>Occupancy:     average occupancy profile of the building over 24 hours for weekdays and weekends</p> <p> Occupancy input </p> </li> </ul> <p>Dedicated pages for material databases where it is possible to define layer compositions for vertical opaque components and display them in 3D </p> Custom component definition by layers"},{"location":"tools-and-services/ecm/#output","title":"OUTPUT","text":"<p>it is possible to view the primary energy, the building's thermal demand, the internal temperature trends, also the temperature visualization according to the groupings provided by EN 16798-1</p> Primary and energy need of building Daily and hourly energy profile Indoor Temperature profile and Temperature category from EN 16798-1"},{"location":"tools-and-services/ecm/#energy-conservation-measures","title":"ENERGY CONSERVATION MEASURES","text":"<p>it is possible to apply different ECMs to the building and evaluate both economic and energy impact to the building. The ECM can be applied to:</p> <pre><code>1)  The facade, changing the layer composition of the vertical opaque components\n2)  The roof, changing the layer composition of the vertical opaque components\n3)  The floor, changing the layer composition of the vertical opaque components\n4)  The windows, changing the layer composition of the vertical transparent components\n5)  The heating system, changing the performance of the generator\n</code></pre> <p>The tool simulates each individual ECM as well as combinations of multiple ECMs, when more than one is selected. In the end, it is possible to evaluate both the energy impact and the economic impact, if the investment cost for the ECM and the energy cost per kWh are provided.</p> <pre><code>&lt;figure markdown=\"span\"&gt;\n</code></pre> <p></p> List of Simulated ECM Energy impact of ECMs Simple pay back time for each ECM"},{"location":"tools-and-services/ecm/#photovoltaic-simulation","title":"Photovoltaic simulation","text":"<p>Photovoltaic Simulation (ECM) and Optimization. Photovoltaics simulation allows the user to assess how much photovoltaics capacity is needed when the energy consumption for heating is covered entirely by an electric generator (heat pump), and additional data is required including i. e., an annual profile of electricity consumption from appliances. The optimization provides a guide for the design of a photovoltaic plus battery energy storage system coupled with a heat pump for different locations and thus different climates in Europe. Through a multi-objective optimization approach applied to the six European geoclusters and applying different solution selection criteria, it is possible to obtain the optimal photovoltaic capacity (kWp) normalized over the thermal consumption of a reference building and the optimal capacity of the battery per kWp of photovoltaic (kWh/kWp) installed. Due to the normalization process, results can be generalized and can be used for designing similar systems in all buildings around Europe. The results must be intended as a support for designers for the early-design phase of such systems or as an initial guess for an iterative process in more advanced evaluations</p>"},{"location":"tools-and-services/ecm/#references","title":"References","text":"<ul> <li>Benchmarking tool</li> </ul>"},{"location":"tools-and-services/eso_A1.2/","title":"Energy System Optimization","text":"<p>Abstract</p> <p>This tool is a software application that, exploiting synergies within MODERATE and SYNAVISION platform, offers a customizable environment for setting up time series data analytics for energy system optimization.</p>"},{"location":"tools-and-services/eso_A1.2/#introduction","title":"Introduction","text":"<p>New building frontiers are taking hold. Increasingly complex systems require greater controls. IoT is becoming an essential element to ensure that the potential of the building defined in the design phase is respected. Continuous commissioning is fundamental to optimize buildings in order to identify the right set-up that guarantees the best level of comfort at the lowest cost.</p> <p>This tool enables the user to control the heating requirement in dependency of a weather forecast. This enables targeted control of the room air temperature so that the user does not have to worry about it. If the weather forecast indicates that a certain temperature limit will be exceeded over the next few days, the room temperature setpoint is lowered as a precaution, thus minimizing the heating requirement.</p> <p>The tool is an optimization skill that works especially in combination with a MyGekko building automation system.</p>"},{"location":"tools-and-services/eso_A1.2/#user-guide","title":"User Guide","text":"<p>The tool is divided into several parts. An IO service must be created in the SYNAVISON platform:</p> <p></p> <p>This enables the import of current weather data for a selected location. From this, the outside air temperature is used as input for the \"Heat Control\" skill in the synavision platform. As a result, this skill outputs a time series that corresponds to the suggested room setpoint temperature.</p> <p></p> <p>This virtual setpoint calculated by the skill is published via MQTT in proprietary MyGekko format by the SYNAVISION platform and picked up by MyGekko influencing the room control.</p>"},{"location":"tools-and-services/eso_A1.2/#input-output","title":"Input / Output","text":"<p>The skill asks for the following input and parameters:</p> <ul> <li>Numerical timeseries ambient air temperature</li> </ul> <p>The following Output data is provided by the skill and available to MODERATE APIs:</p> <ul> <li>Numerical timeseries virtual room temperature setpoint</li> </ul> <p></p>"},{"location":"tools-and-services/eso_A1.2/#user-experience","title":"User experience","text":"<p>In principle the skill is an application without an UI, that runs on SYNAVISION client-server application. In this case, the Digital Test Bench provides the UI functionalities, connects to SYNAVISION platform and to MODERATE APIs. The client, as shown in the screenshot, offers the easy application of the skill following these steps:</p> <ul> <li>Enter a workspace.</li> <li>Create an instance of an open weather import plugin to get the forcasted ambient air temperature</li> <li>Create an instance of the skill \"Heat Control\" by double clicking, naming and saving the instance.</li> <li>Choose the ambient air temperature times series you want to use for control by drag and drop.</li> <li>Save the skill.</li> <li>Create an instance of an proprietary My Gekko Control IO service to publish the setpoint commands in MyGekko JSON format to the respective topics via an export job. (Work in progress. The IO-Service will be developed by synavision soon)</li> </ul>"},{"location":"tools-and-services/eso_A1.2/#application-on-moderate","title":"Application on MODERATE","text":"<p>Within MODERATE, this service can be used to reduce the heat requirement by calculating a virtual room setpoint temperature depending on the predicted outside air temperature. This virtual room setpoint temperature is sent to the building automation system via MQTT so that the setpoint temperature of the room is lowered there.</p>"},{"location":"tools-and-services/eso_A1.2/#references","title":"References","text":"<ul> <li>https://www.my-gekko.com/de/automatisierungstechnik/1-0.html</li> </ul>"},{"location":"tools-and-services/fdf_A1.1/","title":"Fault Detection and Forecasting","text":"<p>Abstract</p> <p>This tool is a software application that, exploiting synergies within MODERATE and SYNAVISION platform, offers a customizable environment for setting up time series data analytics for fault detection and forecasting for optimization of operations, thus enabling predictive maintenance on technical building systems.</p>"},{"location":"tools-and-services/fdf_A1.1/#introduction","title":"Introduction","text":"<p>An effective and efficient monitoring and control system is essential for service providers such as utility companies and ESCOs as well as facility managers. The goal of continuous service without failures is only achievable through a continuous evaluation and diagnosis of the data flow of the monitored system. Fault detection is based on mathematical models of signals and processes. Its application avoids the interruption of processes in buildings systems, and particularly effective when associating the detection with predictive maintenance. </p> <p>This tool enables intelligent monitoring and supervision systems, where the fault is identified in near-real time, and positively contribute to building smartness and energy efficiency. The tool allows users, in particular energy managers and utility managers, to predict and explain system failures based on an improved knowledge of building characteristics and behaviour from MODERATE.</p> <p>The tool relies on core functionalities and services available from the SYNAVISION platform (SYNAVISION.de), which offers a digital test bench for smart buildings based on digital twin models. All skills and described services can connect with the SYNAVISION platform via APIs and other data import and export services, which are already operational and validated.</p>"},{"location":"tools-and-services/fdf_A1.1/#user-guide","title":"User Guide","text":"<p>The tool, a so called skill within SYNAVISION platform, can connect with the MODERATE platform APIs, supporting interoperability with other tools and functionalities. It supports additional data I/O, as specific near real-time data handling. While import handles time series and metadata, exports can include various formats such as dashboards, files etc. for reporting KPIs to the users.</p> <p></p> <p>A skill is defined as an application within the context of SYNAVISION platform, and in particular within SYNAVISION's test bench client-side software for digital twinning. It can import times series data and metadata, handle the data logically and arithmetically and export results. The skill is executed whenever new data is imported into corresponding workspace of the platform. </p> <p></p> <p>Besides the skill engine, the SYNAVISION platform allows for a highly flexible domain-specific language (sand-box model) for additional, individual data handling. </p> <p>There have been developed two different skills for the aim of fault detection (1) and for forecasting (2).</p> <p>The skill for anomaly detection (Anomaly Detection Skill) identifies anomalies in time series and can therefore be used for fault detection purposes. Various algorithms for anomaly detection can be selected by the user. In addition, meaningful metrics are generated, which provide an overview of the general state as well as the recent historical data points regarding occurring anomalies.</p> <p>The skill for forcasting (Forecasting Timeseries Skill) can be used to for prediction of timeseries data. In particular, this skill can generate a time series dataset for a selected period of time in the future based on at least three weeks of historical data (the minimum useful range to generate acceptable results, found empirically) and additional information from MODERATE knowledge. Various algorithms for prediction are available to the user. In addition, meaningful KPIs are generated providing additional information on the data used and the information generated.</p>"},{"location":"tools-and-services/fdf_A1.1/#input-output","title":"Input / Output","text":""},{"location":"tools-and-services/fdf_A1.1/#anomaly-detection-skill","title":"Anomaly Detection Skill","text":"<p>The skill asks for the following input and parameters:</p> <ul> <li>Numerical time series of which a subrange is to be checked for anomalies, including user data as well as time series datasets passed by MODERATE APIs, i.e., from other MODERATE services. The skill can handle any kind of data, i.e., energy metering, mass flow rate, electric current, IEQ monitoring. The type of algorithm, time range and other variables need to be adapted for best performance.</li> <li>Size of the time window to be checked for anomalies (test interval). The specification is made in time steps according to the data point quantization. For example, to test the last day of the time series or the last 96 timesteps of the time series for anomalies at a quarter-hour resolution, a time window size of 96 must be specified).</li> <li>Size of the time window in number of time steps according to the data point quantization to be used for learning the normal process (training interval).</li> <li>Selection of the concrete algorithm to be used to detect the anomalies:</li> <li>Auto Regression</li> <li>Generalized ESD Test</li> <li>Inter Quartile Range</li> <li>Rolling Average</li> <li>Principal Component Analysis.</li> </ul> <p>The algorithmic options provided here have been identified as useful and robust options for generic anomaly detection and shall be applied depending on the type and characteristics of the time series data. The selection of the algorithm needs to be done by an expert depending on individual types of data.</p> <p></p>"},{"location":"tools-and-services/fdf_A1.1/#forecasting-timeseries-skill","title":"Forecasting Timeseries Skill","text":"<p>The skill asks for the following input:</p> <ul> <li>Numerical time series containing the historical data.</li> <li>Size of the time window for which the time series shall be predicted.</li> <li>Size of the time window used as training data.</li> <li>Selection of the concrete algorithm to be used to predict the future behaviour:</li> <li>Holt Winters Additive Trend Additive Season</li> <li>Holt Winters Additive Trend Multiplicative Season</li> <li>Neural Basis expansion analysis</li> <li>Trigonometric seasonality, Trend and Season</li> <li>Trigonometric seasonality, Trend Box-Cox, Trend</li> <li>Trigonometric seasonality, Trend Box-Cox, Trend and Season</li> <li>Trigonometric seasonality, Trend Box-Cox, Trend and 2 Season</li> </ul> <p>As output, the skill delivers:</p> <ul> <li>a forecasted time series</li> </ul> <p>and as indicators for the quality of the prediction:</p> <ul> <li>the Root Mean Squared Error</li> <li>the Mean Squared Error</li> <li>the Mean Absolute Error.</li> </ul>"},{"location":"tools-and-services/fdf_A1.1/#user-experience","title":"User experience","text":"<p>In principle the skill is an application without an UI, that runs on SYNAVISION client-server application. In this case, the Digital Test Bench provides the UI functionalities, connects to SYNAVISION platform and to MODERATE APIs. </p>"},{"location":"tools-and-services/fdf_A1.1/#anomaly-detection-skill_1","title":"Anomaly Detection Skill","text":"<p>The client, as shown in the screenshot, offers the easy application of the skill following these steps:</p> <p></p> <p>Steps:</p> <ul> <li>Enter a workspace.</li> <li>Create an instance of the skill by double clicking, naming and saving the instance.</li> <li>Choose the times series you want to analyse by drag and drop.</li> <li>Enter the length of testing and training period (number of time steps)</li> <li>Choose the algorithm to be applied.</li> <li>Save the skill.</li> </ul> <p>The following screenshot shows a time series (below) and the anomalies detected through a principal component analysis.</p> <p></p>"},{"location":"tools-and-services/fdf_A1.1/#forecasting-timeseries-skill_1","title":"Forecasting Timeseries Skill","text":"<p>The client, as shown in the screenshot, offers the easy application of the skill following these steps:</p> <p></p> <p>Steps:</p> <ul> <li>Enter a workspace.</li> <li>Create an instance of the skill by double clicking, naming and saving the instance.</li> <li>Choose the times series you want to analyse by drag and drop.</li> <li>Enter the length of testing and training period (number of time steps)</li> <li>Choose the algorithm to be applied.</li> <li>Save the skill.</li> </ul> <p>The following screenshot shows the time series (blue) and the predicted times series for the next day (green).</p> <p></p>"},{"location":"tools-and-services/fdf_A1.1/#application-on-moderate","title":"Application on MODERATE","text":""},{"location":"tools-and-services/fdf_A1.1/#anomaly-detection-skill_2","title":"Anomaly Detection Skill","text":"<p>Within MODERATE, this service can be used to identify anomalies in any time series, e.g., weather data, metering data etc. Extended with an auto-correction skill for data enhancement, e.g., for deleting and replacing values, the tool can be used as a powerful \"behind-the-scenes\"-tool to improve data quality and performance of other services within the MODERATE platform.</p>"},{"location":"tools-and-services/fdf_A1.1/#forecasting-timeseries-skill_2","title":"Forecasting Timeseries Skill","text":"<p>Time series prediction can be used to model the future behaviour of a system, e.g., the energy consumption of a building. Knowing the future energy consumption can help to manage energy demand by optimizing the management of storage capacities, e.g., through load shifting or peak shaving. By knowing when a load peak is likely, when energy prices are high (or low) or demand is going to be low (or high), the building management system can decide to increase or reduce the amount of stored energy in buffer tanks or construction and thus avoid reaching peaks or buying energy at high cost. </p>"},{"location":"tools-and-services/fdf_A1.1/#references","title":"References","text":"<ul> <li>https://de.documentation.synavision.de/specification/skill/</li> <li>https://lib.synavision.de/produkt-kategorie/skills/</li> </ul>"},{"location":"tools-and-services/geoclustering_tools/","title":"Geoclustering Tool","text":"<p>Abstract</p> <p>The geoclustering tool is designed to cluster buildings based on their geospatial location and other relevant features. This tool can be used to identify patterns and relationships between buildings, and to better understand the distribution of buildings in a given area. Moreover, the tool allows to perform a sensitivity analysis to evaluate the impact of different clustering parameters on the results. An example of this analysis is the evaluation of different scenarios to study the impact of the trasmittance of the envelope on the energy need of the buildings.</p>"},{"location":"tools-and-services/geoclustering_tools/#introduction","title":"Introduction","text":"<p>The geoclustering tool is a web-based platform that enables users to cluster buildings based on different features, including geospatial location and other relevant information. The tool is built using Python and is designed to be easy to use and understand.  The user can visualize the characteristics of the EPC dataset, filter them according to specifici physical features, and perform a geoclustering analysis.  The resulting clusters are visualized in a dedicated map.</p> <p>Subsequently, the user can perform a sensitivity analysis to evaluate the impact of different clustering parameters on the results. </p> <p>The engine of the tool is availble here:  https://github.com/MODERATE-Project/geoclustering_sensitivity_analysis</p> <p>The user can use the web app or perform a directly analysis using the dedicated repository.</p>"},{"location":"tools-and-services/geoclustering_tools/#workflow","title":"Workflow","text":"Geoclustering workflow"},{"location":"tools-and-services/geoclustering_tools/#user-guide","title":"User Guide","text":"<p>The tool is composed of two main sections, accessible through the navbar in the header. These sections include two main categories:</p> <p>https://tools.eeb.eurac.edu/epc_clustering/piemonte/</p>"},{"location":"tools-and-services/geoclustering_tools/#epc-analysis","title":"EPC - Analysis","text":"<p>Users can view the EPC dataset, filter it according to specifici physical features, and perform a geoclustering analysis.</p> EPC dataset Clustering Potential energy reduction Scenarios"},{"location":"tools-and-services/geoclustering_tools/#synthetic-dataset","title":"Synthetic dataset","text":"<p>In this section, users can visualize the synthetic dataset of the EPCs.</p> Synthetic report of EPC"},{"location":"tools-and-services/lec/","title":"Local Energy Communities Assessment Tool","text":"<p>Abstract</p> <p>The Local Energy Communities (LEC) Assessment Tool is designed to help users identify optimal locations for establishing Local Energy Communities. By analyzing various datasets, including energy consumption, cadastral data, and infrastructure information, the tool provides insights into areas where LECs can be successfully implemented.</p> <p>Using machine learning techniques, the tool processes complex data to evaluate building potential, focusing on solar energy capabilities. Users can interact with a map-based interface, filter buildings, generate reports, and obtain detailed building and cadastral information. The tool aims to facilitate the development of LECs, supporting energy decarbonization and the transition to sustainable energy systems.</p>"},{"location":"tools-and-services/lec/#introduction","title":"Introduction","text":"<p>A Local Energy Community (LEC) is a legal entity where various actors collaborate to meet their energy needs through shared production, transmission, and consumption. LECs play a pivotal role in the transition toward decarbonizing buildings, promoting social interaction, and integrating renewable energy sources. However, one of the main challenges in establishing a successful LEC is identifying the geographic areas where these communities can thrive. This is where the tool comes into play.</p> <p>The tool is designed to solve the complex problem of determining suitable areas for LECs by analyzing diverse datasets, such as energy consumption, cadastral information, and infrastructure details. Using machine learning techniques, it processes this information to provide actionable insights that help users define areas where LECs can be effectively implemented.</p> <p>By enabling the creation of LECs, the tool contributes to a more sustainable energy ecosystem, supporting decarbonization efforts and the wider adoption of renewable energy. It helps streamline energy management, making energy production and consumption more efficient within communities. Users who invest time in learning and using this tool gain the ability to identify optimal locations for LECs, contributing to environmental sustainability and community engagement, while simplifying a typically complex and data-intensive process.</p> <p>The tool is currently accessible at the following URL: https://lec.staging.moderate.cloud/</p>"},{"location":"tools-and-services/lec/#user-guide","title":"User Guide","text":"<p>The main view of the web app shows a map of a country, currently Spain, with all its municipalities. The user can select any municipality to center the screen for close examination. However, only those municipalities with a blue background contain building data.</p> An initial view of the tool, where each polygon represents a municipality in Spain. Only the blue polygons contain data. <p>Currently, only the municipality of Crevillent contains building data.</p> <p>When focused on a municipality that contains data, the bottom of the screen displays a table with the first 10 buildings ranked by solar potential radiation. This is the default sorting criterion, but the user can change it in two ways, which will be explained in the next sections.</p> Selecting a municipality. Notice that additional buttons appear, and the table at the bottom contains data. <p>The main functionalities of the tool are:</p> <ul> <li>Viewing building data in a table format and manipulating its sorting criteria.</li> <li>Viewing building data on a map, with colors representing potential solar radiation. This also provides access to cadastral information.</li> <li>Filtering buildings by drawing a polygon box on the map.</li> <li>Saving or printing a document containing the current state of both the table data and the map view.</li> <li>Generating a personalized report that provides an overview of the municipality and a detailed analysis of the selected buildings.</li> </ul> <p>In the following sections, we describe in detail how the user can achieve these actions.</p>"},{"location":"tools-and-services/lec/#the-map-view","title":"The Map View","text":"<p>As mentioned before, the user can select any municipality on the map. If the selected municipality contains building data, the interface will show additional buttons and gradient colors, as shown in the image below.</p> Zooming in, a color gradient on the map indicates the relative solar potential of the buildings. <p>The user can click on a row in the table to center the view on a specific building, or navigate the map and click on it directly. The selected building will display a diagonal hatch pattern to indicate its location, and a popover on the right side will provide detailed building information and a button to redirect the user to the building's page within the official cadastral website.</p> Building cadaster data <p>Additionally, if the user zooms in enough, the centroids of the buildings will appear. This activates the comparison mode, where the user can hover over any building to see its information.</p> <p>The following image illustrates these functionalities:</p> Analyzing buildings and their solar potential"},{"location":"tools-and-services/lec/#changing-sorting-criteria","title":"Changing Sorting Criteria","text":"<p>As mentioned before, the default sorting is by solar potential radiation. The user can change the order of the table by clicking on the column headers. For more complex sorting, the user can click the \"Sorting criteria\" box to unfold the advanced sorting options.</p> <p>As shown in the image below, the default sorting criterion is 'Total Production.' However, within this panel, users can select any relevant variable, drag it to the desired position, and reorder them. The position of each variable indicates its importance, with higher positions representing greater significance. Each variable is assigned a weight, and the buildings are sorted based on the weighted sum of these variables.</p> Sorting criteria panel, located at the bottom right of the screen <p>After the sorting criteria are defined, the user can click the button to apply the new sorting to the table below.</p>"},{"location":"tools-and-services/lec/#filtering-buildings-by-area","title":"Filtering Buildings by area","text":"<p>One of the key features of the tool is the filtering of buildings. From all the available buildings, the user can select a subset by using the draw mode.</p> <p>To change the tool\u2019s mode, the user can click on the  button, located on the left side of the screen.</p> <p>With this mode selected, the pointer changes to a blue dot, and some view functionalities will be disabled. The user can then click to add several points to form a desired polygon. When the user clicks on the first point again, or double-clicks, the polygon will close, and the tool will filter and display only the buildings inside the polygon.</p> <p>Consequently, the table will also display only the buildings within the polygon, ensuring consistency between both views.</p> Filtering a block of buildings by drawing a polygon over them. <p>To exit the drawing mode, the user can click the draw button again. A button at the bottom of the map allows the user to reset the filter and display all buildings again.</p>"},{"location":"tools-and-services/lec/#filtering-buildings-by-type","title":"Filtering Buildings by type","text":"<p>Users can filter buildings by type using the dropdown menu located beneath the drawing mode button. This menu allows the selection of one or more building types, such as residential, commercial, industrial, and more. The selected types will be highlighted in the dropdown menu.</p> <p>Both filtering options will affect the table, map, and report generation, ensuring that users can focus on specific building characteristics.</p>"},{"location":"tools-and-services/lec/#report-generation","title":"Report Generation","text":"<p>The tool allows users to generate a report detailing the characteristics of the buildings, along with additional data and a brief analysis of the selected municipality and the filtered buildings.</p> <p>To generate the report and save it to their computer, users can click the \"Generate report\" button in the top-right corner of the screen. The report will be generated and automatically downloaded within a few seconds.</p> <p>About the report generation</p> <p>This feature is still in its early stages, so the generated report may not be fully complete.</p>"},{"location":"tools-and-services/lec/#saving-or-printing-the-current-view","title":"Saving or Printing the Current View","text":"<p>The tool allows users to save or print the current view, which includes both the table and the map. This is useful for sharing information or keeping a record of the analysis.</p> <p>To save or print the current view, users can click the \"Print\" button next to the report button. This will generate a document that includes both the table and map views in their current state.</p>"},{"location":"tools-and-services/lec/#references","title":"References","text":"<ul> <li> <p>Access to the tool: https://lec.staging.moderate.cloud/</p> </li> <li> <p>Spanish cadastre website: https://www.sedecatastro.gob.es</p> </li> </ul>"},{"location":"tools-and-services/mea_and_ver/","title":"Measurement and Verification Tool","text":"<p>Abstract</p> <p>The Measurement and Verification Tool is a web application that allows users to perform measurement and verification of building energy performance, using the option C of the International Performance Measurement and Verification Protocol (IPMVP).</p> <p>The Measurement and Verification Tool is a web application that allows users to perform measurement and verification of building energy performance, using the option C of the International Performance Measurement and Verification Protocol (IPMVP).</p> <p>The Option C measures the total energy consumption of a building or plant before and after an energy efficiency intervention, to calculate the total savings obtained.</p> <p>It uses the real measurement of the consumption (e.g. from electricity meters, gas, etc.).</p> <p>It builds a statistical baseline model (typically regression) based on historical data and independent variables such as:</p> <ul> <li>External temperature</li> <li>Working days</li> <li>Occupation hours</li> </ul> <p>It compares the predicted consumption (baseline) with the actual consumption after the intervention, normalizing for conditions.</p>"},{"location":"tools-and-services/mea_and_ver/#workflow","title":"Workflow","text":"Measurement and Verification Tool workflow"},{"location":"tools-and-services/mea_and_ver/#user-guide","title":"User Guide","text":"<p>The tool use the engine developed in the repository: [Measurement and Verification engine](https://github.com/MODERATE-Project/Measurement-and-verification</p> <p>the resulting report is intgerated in the benchmarking web tool:  [Benchmarking and M&amp;V tool](https://tools.eeb.eurac.edu/building_benchmarking/ </p> <p>in the dedicated section: Measurement and Verification</p>"},{"location":"tools-and-services/mea_and_ver/#report","title":"Report","text":"<p>Example of the resulting analysis:</p> Measurement and Verification Tool graph - model 1 Measurement and Verification Tool graph - model 2 <p>an example of the entire report is available here:  Example Report </p>"},{"location":"tools-and-services/qualitycheck/","title":"Quality Check Report Tool","text":"<p>Abstract</p> <p>The MODERATE Quality Check Report Tool is a web-based application designed to validate and analyze Energy Performance Certificates (EPC) in XML format. This tool automates the process of identifying inconsistencies and discrepancies in EPCs by applying a set of predefined validation rules. The application provides immediate feedback through a user-friendly interface, helping users ensure their certificates meet all required standards and specifications.</p>"},{"location":"tools-and-services/qualitycheck/#introduction","title":"Introduction","text":"<p>The MODERATE Quality Check Report Tool was developed to address the need for automated validation of Energy Performance Certificates. By providing instant analysis and detailed feedback, it helps users identify and correct issues in their EPCs before submission, reducing errors and improving the overall quality of energy certification processes.</p> <p>The tool is particularly useful for professionals in the energy sector, including energy auditors, building managers, and regulatory bodies, who require a reliable method to ensure compliance with energy performance standards.</p> <p>The tool is currently accessible at the following URL: http://moderate.five.es:55000</p>"},{"location":"tools-and-services/qualitycheck/#user-guide","title":"User Guide","text":"<p>The MODERATE Quality Check Report Tool provides a straightforward process for validating your Energy Performance Certificates. The application is designed to be intuitive and user-friendly, requiring minimal technical knowledge to operate effectively.</p> <p>To begin using the tool, simply drag and drop your EPC XML file onto the designated area of the interface. The system will automatically process the file and begin the validation process. If the certificate requires additional information, the application will present a series of questions that need to be answered to complete the validation.</p> <p>Once the validation is complete, you'll be presented with a comprehensive view of the validation results. By default, the interface shows only the rules that have been violated. However, you can easily toggle to view all validation rules, including those that have passed and failed, by using the filter options available in the interface.</p> <p>The validation results are presented in a clear, organized manner, showing:</p> <ul> <li>The specific rules that were checked</li> <li>The status of each validation (passed/failed)</li> <li>Detailed information about any issues found</li> <li>The location of problems within your certificate</li> </ul> Initial view of the application. Note the draggable section in the center of the screen"},{"location":"tools-and-services/qualitycheck/#epc-validation-rules","title":"EPC Validation Rules","text":"<p>The tool applies a set of predefined validation rules to the EPC XML file. These rules are designed to ensure that the certificate meets all required standards and specifications. The rules are organized into categories, each with its own set of criteria:</p> <ul> <li>Common Rules: These rules are applied to all EPCs</li> <li>Model Specific Rules: Different rules are applied depending on the EPC model</li> </ul>"},{"location":"tools-and-services/qualitycheck/#rules-status","title":"Rules Status","text":"<p>After each rule has been validated, the tool will provide a status for the rule. The status can be:</p> <ul> <li>Passed: The rule has been successfully validated</li> <li>Error: The rule has not been successfully validated</li> <li>Suspected error: The tool has detected an issue that requires further investigation</li> </ul>"},{"location":"tools-and-services/qualitycheck/#report-generation","title":"Report Generation","text":"<p>The user can generate a report of the validation results. To do so, click on the \"Generate Report\" button. The report will be generated in PDF format.</p> Example of validation results <p>Known Limitations and Issues</p> <p>Please note that this tool is currently a proof of concept. It is not yet fully functional and is still under development. Concurrent users may experience issues.</p>"},{"location":"tools-and-services/qualitycheck/#references","title":"References","text":"<ul> <li>Application URL - Direct access to the MODERATE Quality Check Report Tool</li> <li>EPC Documentation - Spanish Official EPC format documentation</li> <li>Example EPCs - Sample valid EPC files for reference. Access is restricted to the MODERATE team</li> </ul>"},{"location":"tools-and-services/solar-cadastre/","title":"Solar Cadastre","text":"<p>Abstract</p> <p>The Solar Cadastre (SC) is designed to help users calculate the energy potential of their buildings or assess the performance of their solar panels.</p> <p>The tool utilizes different profiles to analyze building data, calculate solar potential, and retrieve geospatial information. Users can interact with the map by selecting their building and performing the two available calculations.</p>"},{"location":"tools-and-services/solar-cadastre/#introduction","title":"Introduction","text":"<p>A Solar Cadastre (SC) is a geospatial registry that provides information about the solar energy potential of buildings and land parcels within a geographical area. It helps users assess the suitability of their properties for solar energy generation.</p> <p>The tool is designed to analyze building data within a city, calculating solar potential using a geospatial system. It provides users with key indicators in percentage format to assess their building's solar potential.</p> <p>Additionally, users can perform a second calculation to evaluate the performance of their solar panels, determining whether they are operating efficiently.</p>"},{"location":"tools-and-services/solar-cadastre/#user-guide","title":"User Guide","text":"<p>The main view of the web application allows users to select the city they want to analyze.</p> City selection screen. Currently, the tool supports only one city in Spain. <p>Currently, only the municipality of Crevillent contains building data.</p> <p>Upon selecting a city, the tool redirects users to an interactive map with various functionalities.</p> Map interface. Currently, only Crevillent, Spain, is available."},{"location":"tools-and-services/solar-cadastre/#key-functionalities","title":"Key Functionalities","text":"<ul> <li>View summarized data of selected buildings by drawing an area on the map.</li> <li>Retrieve building information, calculate energy potential, and assess solar performance, including cadastral details.</li> <li>Calculate the energy potential of a selected building.</li> <li>Perform a solar performance evaluation.</li> <li>Toggle different map layers, such as cadastral buildings, photovoltaic (PV) generation cells, and cadastral data.</li> </ul> <p>The following sections detail how users can utilize these functionalities.</p>"},{"location":"tools-and-services/solar-cadastre/#selecting-an-area-on-the-map","title":"Selecting an Area on the Map","text":"<p>Users can draw a polygon over the buildings on the map to obtain summarized data, including PV energy potential, nominal power, optimal PV installation, potential CO2 emission reductions, and average yield.</p> Drawing an area on the map."},{"location":"tools-and-services/solar-cadastre/#selecting-a-building-on-the-map","title":"Selecting a Building on the Map","text":"<p>Users can select a building using the pin tool. Once selected, detailed building data appears on the left panel, along with raster information.</p> Building selection interface. <p>The tool allows two processes: energy potential calculation and solar performance evaluation.</p>"},{"location":"tools-and-services/solar-cadastre/#calculating-energy-potential","title":"Calculating Energy Potential","text":"<p>A pre-filled form appears, which users can modify if they have more accurate data for their building. Users must select a profile to ensure accurate calculations.</p> Energy potential calculation form. <p>Once the profile is selected and the calculation is initiated, a \"Processing\" message appears, indicating the computation is in progress. This may take a few seconds to minutes.</p> Processing solar potential calculation. <p>After processing, the tool displays the results in a table and graph format.</p> Solar potential calculation results."},{"location":"tools-and-services/solar-cadastre/#solar-performance-check","title":"Solar Performance Check","text":"<p>To return to the building data screen, users can click the back button.</p> <p>To perform the second calculation, users must press the \"Evaluate\" button.</p> Solar performance evaluation. <p>Users must enter the energy generated in the last year.</p> Solar performance evaluation form. <p>After clicking \"Evaluate,\" a processing message appears, and results are displayed after a few seconds.</p> Processing solar performance check. <p>Results include two graphs\u2014one showing cumulative energy output over past years and another displaying the current year's performance.</p> Solar performance results. <p>Users can select a month from a dropdown menu to compare expected vs. actual energy output, then answer a verification question.</p> Monthly performance comparison. <p>If expected and actual outputs match, the system confirms proper functionality.</p> Confirmation of proper solar panel operation. <p>If they do not match, users must indicate \"No\" and select a reason from a predefined list.</p> Selecting a reason for discrepancies. <p>After submission, the tool displays the selected reason.</p> Final confirmation of selected reason."},{"location":"tools-and-services/solar-cadastre/#viewing-layers","title":"Viewing Layers","text":"<p>Users can enable map layers by clicking the first menu option and selecting a layer, such as cadastral buildings or land registry data.</p> Building cadastre layer."},{"location":"tools-and-services/solar-cadastre/#references","title":"References","text":"<ul> <li>Spanish cadastre website: https://www.sedecatastro.gob.es</li> </ul>"}]}